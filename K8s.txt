----------------K8s----------------

google--->go
CNCF --->cloud Native Computing formation

docker conatiner--->appln not accessible---status crash

restart cont or recreate

check logs and perform some action

k8s---> monitor cont 24hrs{restart, redeploy}

Livenessprob---> every 10sec check cont is runnung or not
readiness---> check traffic is going 

reduces manual efforts
manage load
==========================================
ARCHITECTURE:-

MASTER_NODE:
    API-SERVER:
        -cluster gateway    Request--->update or scale the 
    ETCD:
        cluster Brain - store information in key value pair
    SCHEDULER:
        - schedule Healthy node for Pod based on availlability and requirement( resources)

    CONTROLLER M:
        - Detect the Changes and inform to SCHEDULER

WORKER-NODE:
    KUBELET:
        - creation of pod 
        - 
    KUBE-PROXY:
        - managing the network
    CONATINER RUNTIME:

===============================================
WORKFLOW: - 
    1. Api Server---  validate the request 
            store the information insode ETCD
    2. scheduler--> 
            update the information inside ETCD
    3. KUBELET---> create the pod
    4. kube proxy ---> enable the network rule for communication               
    5. controller---> after deployment if something happen wrong 
==================================================================

pod--->1.Frontend. 2.backend, 3.database

if delet 1 IP will Change --->Svc
Secrets
configmaps
pv
pvc
replica
service--->

=========================================================
==========================================================
LINK  [https://youtu.be/JXbTyz1QIHI?si=isIjgaNI1aXgLnfk]
     ------------SELF-HOSTED----------------------

HA-->
Quarom---> what is the minium no. of master node for having
          decision making
          n/2 + 1 = 1.5

1 VM ---> HAProxy
3 VM---> master
2 VM---> Worker


    
Step 1: Prepare the Load Balancer Node
Install HAProxy:

sudo apt-get update
sudo apt-get install -y haproxy
Configure HAProxy: Edit the HAProxy configuration file (/etc/haproxy/haproxy.cfg):

sudo nano /etc/haproxy/haproxy.cfg
Add the following configuration:

frontend kubernetes-frontend
    bind *:6443
    option tcplog
    mode tcp
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    balance roundrobin
    option tcp-check
    server master1 <MASTER1_IP>:6443 check
    server master2 <MASTER2_IP>:6443 check
Restart HAProxy:

sudo systemctl restart haproxy





Step 2: Prepare All Nodes (Masters and Workers)
Install Docker, kubeadm, kubelet, and kubectl:
sudo apt-get update
sudo apt install docker.io -y
sudo chmod 666 /var/run/docker.sock
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubeadm=1.30.0-1.1 kubelet=1.30.0-1.1 kubectl=1.30.0-1.1


Step 3: Initialize the First Master Node
Initialize the first master node:
  
sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_IP:6443" --upload-certs --pod-network-cidr=10.244.0.0/16
Set up kubeconfig for the first master node:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Install Calico network plugin:

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
Install Ingress-NGINX Controller:

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.49.0/deploy/static/provider/baremetal/deploy.yaml



Step 4: Join the Second & third Master Node
Get the join command and certificate key from the first master node:

kubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs | tail -1)
Run the join command on the second master node:

sudo kubeadm join LOAD_BALANCER_IP:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash> --control-plane --certificate-key <certificate-key>
Set up kubeconfig for the second master node:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config




Step 5: Join the Worker Nodes
Get the join command from the first master node:

kubeadm token create --print-join-command
Run the join command on each worker node:

sudo kubeadm join LOAD_BALANCER_IP:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>



Step 6: Verify the Cluster
Check the status of all nodes:

kubectl get nodes
Check the status of all pods:

kubectl get pods --all-namespaces
================================================

Verification

Step 1: Install etcdctl
Install etcdctl using apt:
sudo apt-get update
sudo apt-get install -y etcd-client


Step 2: Verify Etcd Cluster Health
Check the health of the etcd cluster:

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key endpoint health
Check the cluster membership:

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key member list



Step 3: Verify HAProxy Configuration and Functionality
Configure HAProxy Stats:

Add the stats configuration to /etc/haproxy/haproxy.cfg:
listen stats
    bind *:8404
    mode http
    stats enable
    stats uri /
    stats refresh 10s
    stats admin if LOCALHOST
Restart HAProxy:

sudo systemctl restart haproxy
Check HAProxy Stats:

Access the stats page at http://<LOAD_BALANCER_IP>:8404.




Step 4: Test High Availability
Simulate Master Node Failure:

Stop the kubelet service and Docker containers on one of the master nodes to simulate a failure:
sudo systemctl stop kubelet
sudo docker stop $(sudo docker ps -q)
Verify Cluster Functionality:

Check the status of the cluster from a worker node or the remaining master node:

kubectl get nodes
kubectl get pods --all-namespaces
The cluster should still show the remaining nodes as Ready, and the Kubernetes API should be accessible.

HAProxy Routing:

Ensure that HAProxy is routing traffic to the remaining master node. Check the stats page or use curl to test:
curl -k https://<LOAD_BALANCER_IP>:6443/version

Deployment: - 

    Boardgame:
        https://github.com/jaiswaladi246/Boardgame.git


deployment-service.yaml:

    apiVersion: apps/v1
kind: Deployment # Kubernetes resource kind we are creating
metadata:
  name: boardgame-deployment
spec:
  selector:
    matchLabels:
      app: boardgame
  replicas: 2 # Number of replicas that will be created for this deployment
  template:
    metadata:
      labels:
        app: boardgame
    spec:
      containers:
        - name: boardgame
          image: adijaiswal/boardshack:latest # Image that will be used to containers in the cluster
          imagePullPolicy: Always
          ports:
            - containerPort: 8080 # The port that the container is running on in the cluster


---

apiVersion: v1 # Kubernetes API version
kind: Service # Kubernetes resource kind we are creating
metadata: # Metadata of the resource kind we are creating
  name: boardgame-ssvc
spec:
  selector:
    app: boardgame
  ports:
    - protocol: "TCP"
      port: 80
      targetPort: 8080 
  type: LoadBalancer # type of the service.

  ----------------------------------


  k apply -f deployment-service.yaml
  k get all
  k describe pod podname
        check Node IP:NodeportIP--> Run on browser


























